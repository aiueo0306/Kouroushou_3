from feedgen.feed import FeedGenerator
from datetime import datetime, timezone
import os
from urllib.parse import urljoin
import requests
from bs4 import BeautifulSoup

BASE_URL = "https://www.mhlw.go.jp/stf/kinnkyuuhininnyaku.html"


# ==================================================
# RSSç”Ÿæˆï¼ˆUTF-8 BOMä»˜ãã§ä¿å­˜ï¼šWindowsæ–‡å­—åŒ–ã‘å¯¾ç­–ï¼‰
# GUIDã¯URLã®ã¿ï¼ˆè¦æœ›ã©ãŠã‚Šï¼‰
# ==================================================
def generate_rss(items, output_path):
    fg = FeedGenerator()
    fg.title("ç·Šæ€¥é¿å¦Šè–¬ï¼ˆæ›´æ–°ï¼‰")
    fg.link(href=BASE_URL)
    fg.description("åšç”ŸåŠ´åƒçœã€Œç·Šæ€¥é¿å¦Šè–¬ã€ãƒšãƒ¼ã‚¸å†…ãƒªãƒ³ã‚¯ä¸€è¦§")
    fg.language("ja")

    now_utc = datetime.now(timezone.utc)  # æ—¥ä»˜ãŒç„¡ã„ã®ã§å…±é€šã®å–å¾—æ™‚åˆ»ã‚’ä½¿ã†

    for item in items:
        entry = fg.add_entry()
        entry.title(item["title"])
        entry.link(href=item["link"])
        entry.description(item["description"])

        # âœ… GUIDï¼šURLã®ã¿
        entry.guid(item["link"], permalink=False)

        # âœ… pubDateï¼šæ—¥ä»˜ãŒç„¡ã„ã®ã§å–å¾—æ™‚åˆ»
        entry.pubDate(now_utc)

    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    rss_text = fg.rss_str(pretty=True).decode("utf-8")
    with open(output_path, "w", encoding="utf-8-sig", newline="\n") as f:
        f.write(rss_text)


# ==================================================
# div.l-contentMain å†…ã® li ã‚’å…¨éƒ¨å¯¾è±¡ã« href ã‚’æ‹¾ã†
# ==================================================
def fetch_items_all_li_links():
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept-Language": "ja-JP,ja;q=0.9",
    }

    r = requests.get(BASE_URL, headers=headers, timeout=30)
    r.raise_for_status()

    # åšåŠ´çœãƒšãƒ¼ã‚¸ã¯UTF-8æƒ³å®šã§å›ºå®šï¼ˆèª¤åˆ¤å®šå›é¿ï¼‰
    html = r.content.decode("utf-8", errors="replace")
    soup = BeautifulSoup(html, "html.parser")

    items = []
    seen = set()  # URLå˜ä½ã§é‡è¤‡æ’é™¤

    content = soup.select_one("div.l-contentMain")
    if not content:
        return items

    for li in content.select("li"):
        a = li.select_one("a[href]")
        if not a:
            continue

        title = " ".join(a.get_text(" ", strip=True).split())
        href = a.get("href")
        link = urljoin(BASE_URL, href)

        if not title:
            title = link  # ä¿é™ºï¼šãƒ†ã‚­ã‚¹ãƒˆãŒç©ºãªã‚‰URLã‚’ã‚¿ã‚¤ãƒˆãƒ«ã«

        if link in seen:
            continue
        seen.add(link)

        items.append(
            {
                "title": title,
                "link": link,
                "description": title,
            }
        )

    return items


# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==================================================
if __name__ == "__main__":
    print("â–¶ ãƒšãƒ¼ã‚¸HTMLã‚’å–å¾—ä¸­ï¼ˆrequestsï¼‰...")

    try:
        items = fetch_items_all_li_links()
    except Exception as e:
        print("âš  å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ:", e)
        raise SystemExit(1)

    print(f"â–¶ æŠ½å‡ºä»¶æ•°: {len(items)}")
    if not items:
        print("âš  å¯¾è±¡ã® li/a[href] ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    rss_path = "rss_output/kinnkyuuhininnyaku.xml"
    generate_rss(items, rss_path)

    print("\nâœ… RSSãƒ•ã‚£ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†ï¼")
    print(f"ğŸ“„ ä¿å­˜å…ˆ: {rss_path}")
