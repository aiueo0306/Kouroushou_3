from feedgen.feed import FeedGenerator
from datetime import datetime, timezone
import os
from urllib.parse import urljoin
import requests
from bs4 import BeautifulSoup

BASE_URL = "https://www.mhlw.go.jp/stf/shingi/indexshingi.html"

# ==================================================
# RSSç”Ÿæˆï¼ˆUTF-8 BOMä»˜ãã§ä¿å­˜ï¼šWindowsæ–‡å­—åŒ–ã‘å¯¾ç­–ï¼‰
# ==================================================
def generate_rss(items, output_path):
    fg = FeedGenerator()
    fg.title("å¯©è­°ä¼šãƒ»ç ”ç©¶ä¼šç­‰ï¼ˆNEWï¼‰")
    fg.link(href=BASE_URL)
    fg.description("åšç”ŸåŠ´åƒçœ å¯©è­°ä¼šãƒ»ç ”ç©¶ä¼šç­‰ã®NEWæ›´æ–°ã®ã¿")
    fg.language("ja")

    for item in items:
        entry = fg.add_entry()
        entry.title(item["title"])
        entry.link(href=item["link"])
        entry.description(item["description"])
        entry.guid(item["link"], permalink=False)

        # pubDate ãŒå–ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ã†
        if item.get("pubdate"):
            dt = datetime.fromisoformat(item["pubdate"]).replace(tzinfo=timezone.utc)
            entry.pubDate(dt)
        else:
            entry.pubDate(datetime.now(timezone.utc))

    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # FeedGeneratorã¯UTF-8 bytesã‚’è¿”ã™ â†’ BOMä»˜ãã§ä¿å­˜
    rss_text = fg.rss_str(pretty=True).decode("utf-8")
    with open(output_path, "w", encoding="utf-8-sig", newline="\n") as f:
        f.write(rss_text)


# ==================================================
# NEWé …ç›®å–å¾—ï¼ˆspan.m-listLink__link å˜ä½ã§æ­£ç¢ºåˆ¤å®šï¼‰
# ==================================================
def fetch_items_new_only():
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept-Language": "ja-JP,ja;q=0.9",
    }

    r = requests.get(BASE_URL, headers=headers, timeout=30)
    r.raise_for_status()

    # åšåŠ´çœãƒšãƒ¼ã‚¸ã¯UTF-8ãªã®ã§å›ºå®šã§OK
    html = r.content.decode("utf-8", errors="replace")
    soup = BeautifulSoup(html, "html.parser")

    items = []
    seen = set()

    # â˜… é‡è¦ï¼šspan.m-listLink__link å˜ä½ã§ NEW åˆ¤å®š
    for span in soup.select("span.m-listLink__link"):

        # ã“ã® span è‡ªä½“ã« NEW ã‚¢ã‚¤ã‚³ãƒ³ãŒã‚ã‚‹ã‹ï¼Ÿ
        if not span.select_one(".m-icnNew, .toggleIcnNew"):
            continue

        a = span.select_one("a[href]")
        if not a:
            continue

        title = " ".join(a.get_text(strip=True).split())
        link = urljoin(BASE_URL, a.get("href"))

        # æ›´æ–°æ—¥ï¼ˆ<time datetime="YYYY-MM-DD">ï¼‰
        time_tag = span.select_one("time[datetime]")
        pubdate = time_tag["datetime"] if time_tag else None

        description = title
        if pubdate:
            description = f"{title}ï¼ˆæ›´æ–°æ—¥: {pubdate}ï¼‰"

        # é‡è¤‡é˜²æ­¢
        if link in seen:
            continue
        seen.add(link)

        items.append({
            "title": title,
            "link": link,
            "description": description,
            "pubdate": pubdate,
        })

    return items


# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==================================================
if __name__ == "__main__":
    print("â–¶ ãƒšãƒ¼ã‚¸HTMLã‚’å–å¾—ä¸­ï¼ˆrequestsï¼‰...")

    try:
        items = fetch_items_new_only()
    except Exception as e:
        print("âš  å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ:", e)
        raise SystemExit(1)

    print(f"â–¶ NEWæŠ½å‡ºä»¶æ•°: {len(items)}")

    if not items:
        print("âš  NEWã«è©²å½“ã™ã‚‹é …ç›®ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    rss_path = "rss_output/shingi_new.xml"
    generate_rss(items, rss_path)

    print("\nâœ… RSSãƒ•ã‚£ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†ï¼")
    print(f"ğŸ“„ ä¿å­˜å…ˆ: {rss_path}")
