from feedgen.feed import FeedGenerator
from datetime import datetime, timezone
import os
from urllib.parse import urljoin
import requests
from bs4 import BeautifulSoup

BASE_URL = "https://www.mhlw.go.jp/stf/shingi/indexshingi.html"


# ==================================================
# RSSç”Ÿæˆï¼ˆUTF-8 BOMä»˜ãã§ä¿å­˜ï¼šWindowsæ–‡å­—åŒ–ã‘å¯¾ç­–ï¼‰
# GUIDã¯ URL + æ›´æ–°æ—¥ï¼ˆåŒä¸€URLã§ã‚‚æ›´æ–°æ—¥ãŒå¤‰ã‚ã‚Œã°åˆ¥ã‚¢ã‚¤ãƒ†ãƒ æ‰±ã„ã«ãªã‚Šã‚„ã™ã„ï¼‰
# ==================================================
def generate_rss(items, output_path):
    fg = FeedGenerator()
    fg.title("å¯©è­°ä¼šãƒ»ç ”ç©¶ä¼šç­‰ï¼ˆNEWï¼‰")
    fg.link(href=BASE_URL)
    fg.description("åšç”ŸåŠ´åƒçœ å¯©è­°ä¼šãƒ»ç ”ç©¶ä¼šç­‰ã®NEWæ›´æ–°ã®ã¿")
    fg.language("ja")

    for item in items:
        entry = fg.add_entry()
        entry.title(item["title"])
        entry.link(href=item["link"])
        entry.description(item["description"])

        # âœ… GUIDï¼šURLã ã‘ã ã¨æ—¢èª­å›ºå®šã«ãªã‚ŠãŒã¡ãªã®ã§ã€ŒURL#æ—¥ä»˜ã€ã«ã™ã‚‹
        # pubdateãŒç„¡ã„å ´åˆã¯URLã®ã¿ï¼ˆä¿é™ºï¼‰
        if item.get("pubdate"):
            guid = f'{item["link"]}#{item["pubdate"]}'
        else:
            guid = item["link"]
        entry.guid(guid, permalink=False)

        # âœ… pubDateï¼šãƒšãƒ¼ã‚¸å´ã®æ›´æ–°æ—¥ã‚’å„ªå…ˆ
        if item.get("pubdate"):
            # time datetime="YYYY-MM-DD" ã‚’ UTC ã® 00:00 ã¨ã—ã¦æ‰±ã†
            dt = datetime.fromisoformat(item["pubdate"]).replace(tzinfo=timezone.utc)
            entry.pubDate(dt)
        else:
            entry.pubDate(datetime.now(timezone.utc))

    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # FeedGeneratorã¯UTF-8 bytesã‚’è¿”ã™ â†’ BOMä»˜ãã§ä¿å­˜ï¼ˆãƒ¡ãƒ¢å¸³ã§ã‚‚æ–‡å­—åŒ–ã‘ã—ãªã„ï¼‰
    rss_text = fg.rss_str(pretty=True).decode("utf-8")
    with open(output_path, "w", encoding="utf-8-sig", newline="\n") as f:
        f.write(rss_text)


# ==================================================
# NEWé …ç›®å–å¾—ï¼ˆspan.m-listLink__link å˜ä½ã§æ­£ç¢ºåˆ¤å®šï¼‰
# ==================================================
def fetch_items_new_only():
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept-Language": "ja-JP,ja;q=0.9",
    }

    r = requests.get(BASE_URL, headers=headers, timeout=30)
    r.raise_for_status()

    # åšåŠ´çœãƒšãƒ¼ã‚¸ã¯UTF-8æƒ³å®šã§å›ºå®šï¼ˆèª¤åˆ¤å®šå›é¿ï¼‰
    html = r.content.decode("utf-8", errors="replace")
    soup = BeautifulSoup(html, "html.parser")

    items = []
    seen = set()  # ã€ŒURL#æ—¥ä»˜ã€ã§é‡è¤‡æ’é™¤ã™ã‚‹

    # â˜… é‡è¦ï¼šspan.m-listLink__link å˜ä½ã§ NEW åˆ¤å®š
    for span in soup.select("span.m-listLink__link"):

        # ã“ã® span è‡ªä½“ã« NEW ã‚¢ã‚¤ã‚³ãƒ³ãŒã‚ã‚‹ã‹ï¼Ÿ
        if not span.select_one(".m-icnNew, .toggleIcnNew"):
            continue

        a = span.select_one("a[href]")
        if not a:
            continue

        title = " ".join(a.get_text(" ", strip=True).split())
        link = urljoin(BASE_URL, a.get("href"))

        # æ›´æ–°æ—¥ï¼ˆ<time datetime="YYYY-MM-DD">ï¼‰
        time_tag = span.select_one("time[datetime]")
        pubdate = time_tag["datetime"] if time_tag else None

        description = title
        if pubdate:
            description = f"{title}ï¼ˆæ›´æ–°æ—¥: {pubdate}ï¼‰"

        # âœ… é‡è¤‡æ’é™¤ï¼šGUIDã«åˆã‚ã›ã¦ URL#pubdate ã‚’ã‚­ãƒ¼ã«ã™ã‚‹
        key = f"{link}#{pubdate}" if pubdate else link
        if key in seen:
            continue
        seen.add(key)

        items.append({
            "title": title,
            "link": link,
            "description": description,
            "pubdate": pubdate,  # "YYYY-MM-DD" or None
        })

    return items


# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==================================================
if __name__ == "__main__":
    print("â–¶ ãƒšãƒ¼ã‚¸HTMLã‚’å–å¾—ä¸­ï¼ˆrequestsï¼‰...")

    try:
        items = fetch_items_new_only()
    except Exception as e:
        print("âš  å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ:", e)
        raise SystemExit(1)

    print(f"â–¶ NEWæŠ½å‡ºä»¶æ•°: {len(items)}")
    if not items:
        print("âš  NEWã«è©²å½“ã™ã‚‹é …ç›®ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    rss_path = "rss_output/shingi_new.xml"
    generate_rss(items, rss_path)

    print("\nâœ… RSSãƒ•ã‚£ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†ï¼")
    print(f"ğŸ“„ ä¿å­˜å…ˆ: {rss_path}")
